{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7725aa",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "687ca458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "import os, sys, re, torch, json, glob, argparse, gc, ast, pickle, requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tokenizers import AddedToken\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from scripts.formatting_results import *\n",
    "from scripts.bert_filtering import *\n",
    "from scripts.negation import *\n",
    "from scripts.prompting import *\n",
    "from scripts.utils import *\n",
    "from scripts.llama_vision_engine import *\n",
    "# from scripts.llava_med_engine import *\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cea0f08",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adefee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shard_dict(merged_output, index, num_shards=30):\n",
    "    \"\"\"\n",
    "    Split a dictionary into `num_shards` parts using stable modulo sharding.\n",
    "    If index == num_shards, return the remaining (unassigned) keys.\n",
    "    \"\"\"\n",
    "    assert 0 <= index < num_shards\n",
    "\n",
    "    keys = sorted(merged_output.keys())  # deterministic order\n",
    "\n",
    "    shard_keys = [k for i, k in enumerate(keys) if i % num_shards == index]\n",
    "\n",
    "    return {k: merged_output[k] for k in shard_keys}\n",
    "\n",
    "\n",
    "class PhenoGPT2Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Minimal dataset wrapper to allow DataLoader prefetching.\n",
    "    NOTE: No logic changesâ€”this only changes how items are fed into the loop.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_input):\n",
    "        self.data_input = data_input\n",
    "        self.keys = list(data_input.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        k = self.keys[idx]\n",
    "        return k, self.data_input[k]\n",
    "\n",
    "\n",
    "def _collate_single(batch):\n",
    "    # batch is a list of length 1 (batch_size=1)\n",
    "    return batch[0]\n",
    "\n",
    "\n",
    "def build_llm(args):\n",
    "    ##set up model\n",
    "    #Model\n",
    "    if args.model_dir:\n",
    "        model_id = args.model_dir\n",
    "    else:\n",
    "        model_id = os.getcwd() + '/models/phenogpt2'\n",
    "\n",
    "    if args.lora:\n",
    "        peft_config = PeftConfig.from_pretrained(model_id)\n",
    "        # Get path to this file (inference.py)\n",
    "        current_file = os.path.abspath(__file__)\n",
    "\n",
    "        # Get path to phenogpt2 root (go up 2 levels: scripts/ -> phenogpt2/)\n",
    "        project_root = os.path.dirname(current_file)\n",
    "\n",
    "        # Get path to hpo_aware_pretrain\n",
    "        hpo_aware_pretrain_dir = os.path.join(project_root, \"models\", \"hpo_aware_pretrain\")\n",
    "\n",
    "        base_model_name = peft_config.base_model_name_or_path if os.path.isfile(peft_config.base_model_name_or_path) else hpo_aware_pretrain_dir\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            attn_implementation= args.attn_implementation\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(model, model_id)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            attn_implementation= args.attn_implementation\n",
    "        )\n",
    "\n",
    "    #Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast = True)\n",
    "    model.eval()\n",
    "\n",
    "    config = model.config\n",
    "    if config.model_type == 'llama':\n",
    "        tokenizer.chat_template = tokenizer.chat_template = \"\"\"{% for message in messages %}\n",
    "    {% if message['role'] == 'system' %}\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    {{ message['content'] }}<|eot_id|>\n",
    "    {% elif message['role'] == 'user' %}\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    {{ message['content'] }}<|eot_id|>\n",
    "    {% elif message['role'] == 'assistant' %}\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    {{ message['content'] }}<|eot_id|>\n",
    "    {% endif %}\n",
    "    {% endfor %}\n",
    "    {% if add_generation_prompt %}\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    {% endif %}\n",
    "    \"\"\"\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def build_negation(args):\n",
    "    if args.negation:\n",
    "        negation_tokenizer = AutoTokenizer.from_pretrained(args.negation_model, use_fast = True)\n",
    "        negation_model = AutoModelForCausalLM.from_pretrained(\n",
    "            args.negation_model,\n",
    "            dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            attn_implementation= args.attn_implementation\n",
    "        )\n",
    "        emb_model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n",
    "    else:\n",
    "        emb_model = None\n",
    "        negation_tokenizer = None\n",
    "        negation_model = None\n",
    "\n",
    "    return negation_model, negation_tokenizer, emb_model\n",
    "\n",
    "\n",
    "def infer_modes(args, data_input):\n",
    "    # Determine processing mode\n",
    "    use_text = use_vision = False\n",
    "    if args.text_only:\n",
    "        use_text = True\n",
    "    elif args.vision_only:\n",
    "        use_vision = True\n",
    "    else:\n",
    "        # Automatically infer mode based on data\n",
    "        for dt in data_input.values():\n",
    "            if pd.notnull(dt.get('clinical_note')): use_text = True\n",
    "            if pd.notnull(dt.get('image')): use_vision = True\n",
    "            if use_text and use_vision:\n",
    "                break  # no need to continue scanning\n",
    "    return use_text, use_vision\n",
    "\n",
    "\n",
    "def build_vision(args, use_vision):\n",
    "    # Vision model setup (only if vision is enabled)\n",
    "    print(f\"use_vision: {use_vision}\")\n",
    "    if use_vision:\n",
    "        phenogpt2_vision = LLaMA_Generator(os.getcwd() + \"/models/llama-vision-phenogpt2\")\n",
    "        # vision_model = args.vision.lower() if args.vision else \"llava-med\"\n",
    "        # if vision_model == \"llava-med\":\n",
    "        #     phenogpt2_vision = LLaVA_Generator(os.getcwd() + \"/models/llava-med-phenogpt2\")\n",
    "        # elif vision_model == \"llama-vision\":\n",
    "        #     phenogpt2_vision = LLaMA_Generator(os.getcwd() + \"/models/llama-vision-phenogpt2\")\n",
    "        # else:\n",
    "        #     raise ValueError(f\"Unsupported vision model '{vision_model}'. Use 'llava-med' or 'llama-vision'.\")\n",
    "        return phenogpt2_vision\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_one_item(index, dt, data_input, args, model, tokenizer, phenogpt2_vision,\n",
    "                     use_text, use_vision, bert_tokenizer, bert_model,\n",
    "                     negation, negation_model, negation_tokenizer, emb_model, wc):\n",
    "    all_responses = {}\n",
    "    all_responses[index] = {}\n",
    "\n",
    "    if use_text:\n",
    "        text = data_input[index]['clinical_note'].lower()\n",
    "        if wc != 0:\n",
    "            all_chunks = chunking_documents(text, bert_tokenizer, bert_model, word_count = wc)\n",
    "        else:\n",
    "            all_chunks = [text]\n",
    "        temp_response = {}\n",
    "        for para_id, chunk in enumerate(all_chunks):\n",
    "            chunk = chunk.replace(\"'\", \"\"). replace('\"', '')\n",
    "            if len(all_chunks) > 1:\n",
    "                pred_label = predict_label(bert_tokenizer, bert_model, {\"text\":chunk})\n",
    "            else: # in case users only want to use the whole note for testing\n",
    "                pred_label = 'INFORMATIVE'\n",
    "            if pred_label == 'INFORMATIVE':\n",
    "                # Try first attempt\n",
    "                response = generate_output(model, tokenizer, chunk, temperature = 0.3, max_new_tokens = 3000, device = device)\n",
    "                try:\n",
    "                    final_response, complete_check = valid_json(response)\n",
    "                    phenos = final_response.get(\"phenotypes\", {})\n",
    "                    if not isinstance(phenos, dict) or len(phenos) == 0:\n",
    "                        raise ValueError(\"Empty or invalid phenotype dict\")\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        response = generate_output(model, tokenizer, chunk, temperature=0.4, max_new_tokens=5000, device = device)\n",
    "                        final_response, complete_check = valid_json(response)\n",
    "                        phenos = final_response.get(\"phenotypes\", {})\n",
    "                        if not isinstance(phenos, dict) or len(phenos) == 0:\n",
    "                            raise ValueError(\"Empty or invalid phenotype dict after retry\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error: {e}\", flush = True)\n",
    "                        final_response = {'error_response': response}\n",
    "                        final_response['pid'] = data_input[index].get('pid', data_input[index].get('pmid', 'unknown'))\n",
    "                        temp_response[para_id] = final_response\n",
    "                        continue  # move to the next item\n",
    "                if negation:\n",
    "                    print('Starting detecting negation')\n",
    "                    try:\n",
    "                        negation_response = negation_detection(negation_model, negation_tokenizer, chunk, final_response, device = device, max_new_tokens = 10000)\n",
    "                        final_response = process_negation(final_response, negation_response, complete_check, emb_model)\n",
    "                    except:\n",
    "                        final_response['filtered_phenotypes'] = {}\n",
    "                else:\n",
    "                    final_response['filtered_phenotypes'] = {}\n",
    "                # if seen <= 10: ## You can comment this out for logging some early results\n",
    "                #     if len(final_response['filtered_phenotypes']) > 0:\n",
    "                #         print(final_response['filtered_phenotypes'], flush = True)\n",
    "                #         print(final_response['negation_analysis'], flush = True)\n",
    "                #     else:\n",
    "                #         print(final_response['negation_analysis'], flush = True)\n",
    "                temp_response[para_id] = final_response\n",
    "        if len(temp_response) > 1: # if splitting notes into multiple chunks, now merge all\n",
    "            all_responses[index]['text'] = merge_outputs(temp_response)\n",
    "        else:\n",
    "            temp_value = list(temp_response.values())\n",
    "            if len(temp_value) > 0:\n",
    "                all_responses[index]['text'] = temp_value[0] # use the whole note as input\n",
    "            else:\n",
    "                all_responses[index]['text'] = {}\n",
    "    else:\n",
    "        all_responses[index]['text'] = {}\n",
    "\n",
    "    if use_vision:\n",
    "        vision_phenotypes = phenogpt2_vision.generate_descriptions(dt['image'])\n",
    "        phen2hpo = generate_output(model, tokenizer, vision_phenotypes, temperature = 0.4, max_new_tokens = 1024, device = device)\n",
    "        phen2hpo = \"{'demographics': {'age': '\" + phen2hpo\n",
    "        phen2hpo = valid_json(phen2hpo)\n",
    "        phen2hpo = phen2hpo.get(\"phenotypes\", {})\n",
    "        try:\n",
    "            phen2hpo = {phen:hpo_dict['HPO_ID'] for phen,hpo_dict in phen2hpo.items()}\n",
    "        except:\n",
    "            phen2hpo = {}\n",
    "        all_responses[index]['image'] = phen2hpo\n",
    "    else:\n",
    "        all_responses[index]['image'] = {}\n",
    "\n",
    "    return all_responses[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae51dd",
   "metadata": {},
   "source": [
    "## Set Up Your Input here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df38bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input & Output Directory\n",
    "input_dir = \"/home/nguyenqm/projects/PhenoGPT2/testing/phenotagger/input/GSC+/\"\n",
    "output_dir = \"/home/nguyenqm/projects/github/PhenoGPT2/phenogpt2_qwen3_ehr_8b_ft_nofilter/model/evaluations/GSC+_sample\"\n",
    "\n",
    "## Directory to the PhenoGPT2 fine-tuned weights\n",
    "model_dir = \"/home/nguyenqm/projects/github/PhenoGPT2/phenogpt2_qwen3_ehr_8b_ft_nofilter/model\"\n",
    "\n",
    "## Flash Attention helps faster inference and lower GPU memory but it may not work for ARM-based system. Use \"spda\" or \"eager\" instead\n",
    "attn_implementation='flash_attention_2'\n",
    "\n",
    "## Provide the directory to LoRA weights if available; otherwise set False\n",
    "lora=False\n",
    "\n",
    "## Specify if you want to remove false positives (highly recommended)\n",
    "negation=True\n",
    "\n",
    "## Specify the NEGATION model name\n",
    "negation_model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "## This is always on! (unless you want to run vision analysis only)\n",
    "text_only=True\n",
    "\n",
    "## This is always off! (unless you want to run vision analysis)\n",
    "vision_only=False\n",
    "\n",
    "## Specify vision model: llama-vision, qwen-vision, and llava-med\n",
    "vision='llama-vision'\n",
    "\n",
    "## Chunking word per paragraph (recommend to use 300 for long clinical notes); otherwise keep it at 0!\n",
    "wc = 0\n",
    "\n",
    "## If you want to run multiple instances of results (mostly use for SLURM JOB ARRAY)\n",
    "index = 0\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    input=input_dir,\n",
    "    output=output_dir,\n",
    "    model_dir=model_dir,\n",
    "    lora=lora,\n",
    "    index=index,\n",
    "    negation=negation,\n",
    "    negation_model=negation_model_name,\n",
    "    attn_implementation=attn_implementation,\n",
    "    text_only=text_only,\n",
    "    vision_only=vision_only,\n",
    "    vision=vision,\n",
    "    wc=wc\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f43b62",
   "metadata": {},
   "source": [
    "## Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e00d8e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138c7e31a1d14e8b9b460093e2078fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/399 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f2a59d574a4447a21ae29ee567ef53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f703d915eb42afa101e855a235f1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Load PhenoGPT2\n",
    "model, tokenizer = build_llm(args)\n",
    "## Load Negation pipeline\n",
    "negation_model, negation_tokenizer, emb_model = build_negation(args)\n",
    "\n",
    "## Run BERT model for filtering non-phenotypic chunks\n",
    "wc = args.wc\n",
    "if wc != 0:\n",
    "    bert_tokenizer, bert_model = bert_init(local_dir = \"./models/bert_filtering/\")\n",
    "else:\n",
    "    bert_tokenizer, bert_model = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad5d585",
   "metadata": {},
   "source": [
    "## Start PhenoGPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7f966ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start phenogpt2\n",
      "/home/nguyenqm/projects/github/PhenoGPT2/phenogpt2_qwen3_ehr_8b_ft_nofilter/model/evaluations/GSC+_sample\n",
      "/home/nguyenqm/projects/github/PhenoGPT2/phenogpt2_qwen3_ehr_8b_ft_nofilter/model/evaluations/GSC+_sample/phenogpt2_rep0.pkl\n",
      "use_vision: False\n"
     ]
    }
   ],
   "source": [
    "print('start phenogpt2')\n",
    "output_dir = args.output\n",
    "\n",
    "print(output_dir)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "## Process Input:\n",
    "data_input = read_input(args.input)\n",
    "\n",
    "# Load extracted results\n",
    "out_path = f\"{args.output}/phenogpt2_rep{args.index}.pkl\"\n",
    "print(out_path, flush=True)\n",
    "\n",
    "use_text, use_vision = infer_modes(args, data_input)\n",
    "\n",
    "phenogpt2_vision = build_vision(args, use_vision)\n",
    "\n",
    "i = args.index\n",
    "negation = args.negation\n",
    "all_responses = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4877060a",
   "metadata": {},
   "source": [
    "## Running PhenoGPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d372197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# DataLoader wrapper (prefetch)\n",
    "# ----------------------------\n",
    "dataset = PhenoGPT2Dataset(data_input)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=5,\n",
    "    shuffle=False,\n",
    "    num_workers=min(8, max(0, (os.cpu_count() or 4) - 1)),\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True if (os.cpu_count() or 0) > 1 else False,\n",
    "    collate_fn=_collate_single,\n",
    "    prefetch_factor=4 if (os.cpu_count() or 0) > 1 else None,\n",
    ")\n",
    "\n",
    "seen=0\n",
    "for index, dt in tqdm(loader):\n",
    "    all_responses[index] = {}\n",
    "    result = process_one_item(\n",
    "        index=index,\n",
    "        dt=dt,\n",
    "        data_input=data_input,\n",
    "        args=args,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        phenogpt2_vision=phenogpt2_vision,\n",
    "        use_text=use_text,\n",
    "        use_vision=use_vision,\n",
    "        bert_tokenizer=bert_tokenizer,\n",
    "        bert_model=bert_model,\n",
    "        negation=negation,\n",
    "        negation_model=negation_model,\n",
    "        negation_tokenizer=negation_tokenizer,\n",
    "        emb_model=emb_model,\n",
    "        wc=wc\n",
    "    )\n",
    "    all_responses[index] = result\n",
    "#     if seen <= 10:\n",
    "#         print(all_responses[index], flush=True)\n",
    "    seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45386b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{out_path}', 'wb') as f:\n",
    "    pickle.dump(all_responses, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phenogpt2_user",
   "language": "python",
   "name": "phenogpt2_user"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
