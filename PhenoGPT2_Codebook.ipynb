{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7725aa",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "687ca458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "import os, sys, re, torch, json, glob, argparse, gc, ast, pickle, requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tokenizers import AddedToken\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from scripts.formatting_results import *\n",
    "from scripts.bert_filtering import *\n",
    "from scripts.negation import *\n",
    "from scripts.prompting import *\n",
    "from scripts.utils import *\n",
    "from scripts.llama_vision_engine import *\n",
    "# from scripts.llava_med_engine import *\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cea0f08",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adefee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shard_dict(merged_output, index, num_shards=30):\n",
    "    \"\"\"\n",
    "    Split a dictionary into `num_shards` parts using stable modulo sharding.\n",
    "    If index == num_shards, return the remaining (unassigned) keys.\n",
    "    \"\"\"\n",
    "    assert 0 <= index < num_shards\n",
    "\n",
    "    keys = sorted(merged_output.keys())  # deterministic order\n",
    "\n",
    "    shard_keys = [k for i, k in enumerate(keys) if i % num_shards == index]\n",
    "\n",
    "    return {k: merged_output[k] for k in shard_keys}\n",
    "\n",
    "\n",
    "class PhenoGPT2Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Minimal dataset wrapper to allow DataLoader prefetching.\n",
    "    NOTE: No logic changes—this only changes how items are fed into the loop.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_input):\n",
    "        self.data_input = data_input\n",
    "        self.keys = list(data_input.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        k = self.keys[idx]\n",
    "        return k, self.data_input[k]\n",
    "\n",
    "\n",
    "def _collate(batch):\n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "def build_llm(args):\n",
    "    ##set up model\n",
    "    #Model\n",
    "    if args.model_dir:\n",
    "        model_id = args.model_dir\n",
    "    else:\n",
    "        model_id = os.getcwd() + '/models/phenogpt2'\n",
    "\n",
    "    if args.lora:\n",
    "        peft_config = PeftConfig.from_pretrained(model_id)\n",
    "        # Get path to this file (inference.py)\n",
    "        current_file = os.path.abspath(__file__)\n",
    "\n",
    "        # Get path to phenogpt2 root (go up 2 levels: scripts/ -> phenogpt2/)\n",
    "        project_root = os.path.dirname(current_file)\n",
    "\n",
    "        # Get path to hpo_aware_pretrain\n",
    "        hpo_aware_pretrain_dir = os.path.join(project_root, \"models\", \"hpo_aware_pretrain\")\n",
    "\n",
    "        base_model_name = peft_config.base_model_name_or_path if os.path.isfile(peft_config.base_model_name_or_path) else hpo_aware_pretrain_dir\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            attn_implementation= args.attn_implementation\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(model, model_id)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            attn_implementation= args.attn_implementation\n",
    "        )\n",
    "\n",
    "    #Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast = True)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.eval()\n",
    "\n",
    "    config = model.config\n",
    "    if config.model_type == 'llama':\n",
    "        tokenizer.chat_template = tokenizer.chat_template = \"\"\"{% for message in messages %}\n",
    "    {% if message['role'] == 'system' %}\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    {{ message['content'] }}<|eot_id|>\n",
    "    {% elif message['role'] == 'user' %}\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    {{ message['content'] }}<|eot_id|>\n",
    "    {% elif message['role'] == 'assistant' %}\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    {{ message['content'] }}<|eot_id|>\n",
    "    {% endif %}\n",
    "    {% endfor %}\n",
    "    {% if add_generation_prompt %}\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    {% endif %}\n",
    "    \"\"\"\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def build_negation(args):\n",
    "    if args.negation:\n",
    "        negation_tokenizer = AutoTokenizer.from_pretrained(args.negation_model, use_fast = True)\n",
    "        negation_tokenizer.padding_side = \"left\"\n",
    "        if negation_tokenizer.pad_token_id is None:\n",
    "            negation_tokenizer.pad_token = negation_tokenizer.eos_token\n",
    "        negation_model = AutoModelForCausalLM.from_pretrained(\n",
    "            args.negation_model,\n",
    "            dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            attn_implementation= args.attn_implementation\n",
    "        )\n",
    "        emb_model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n",
    "    else:\n",
    "        emb_model = None\n",
    "        negation_tokenizer = None\n",
    "        negation_model = None\n",
    "\n",
    "    return negation_model, negation_tokenizer, emb_model\n",
    "\n",
    "\n",
    "def infer_modes(args, data_input):\n",
    "    # Determine processing mode\n",
    "    use_text = use_vision = False\n",
    "    if args.text_only:\n",
    "        use_text = True\n",
    "    elif args.vision_only:\n",
    "        use_vision = True\n",
    "    else:\n",
    "        # Automatically infer mode based on data\n",
    "        for dt in data_input.values():\n",
    "            if pd.notnull(dt.get('clinical_note')): use_text = True\n",
    "            if pd.notnull(dt.get('image')): use_vision = True\n",
    "            if use_text and use_vision:\n",
    "                break  # no need to continue scanning\n",
    "    return use_text, use_vision\n",
    "\n",
    "\n",
    "def build_vision(args, use_vision):\n",
    "    # Vision model setup (only if vision is enabled)\n",
    "    print(f\"use_vision: {use_vision}\")\n",
    "    if use_vision:\n",
    "        phenogpt2_vision = LLaMA_Generator(os.getcwd() + \"/models/llama-vision-phenogpt2\")\n",
    "        # vision_model = args.vision.lower() if args.vision else \"llava-med\"\n",
    "        # if vision_model == \"llava-med\":\n",
    "        #     phenogpt2_vision = LLaVA_Generator(os.getcwd() + \"/models/llava-med-phenogpt2\")\n",
    "        # elif vision_model == \"llama-vision\":\n",
    "        #     phenogpt2_vision = LLaMA_Generator(os.getcwd() + \"/models/llama-vision-phenogpt2\")\n",
    "        # else:\n",
    "        #     raise ValueError(f\"Unsupported vision model '{vision_model}'. Use 'llava-med' or 'llama-vision'.\")\n",
    "        return phenogpt2_vision\n",
    "    return None\n",
    "\n",
    "def process_one_batch_text(\n",
    "    batch,\n",
    "    data_input,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    bert_tokenizer,\n",
    "    bert_model,\n",
    "    negation,\n",
    "    negation_model,\n",
    "    negation_tokenizer,\n",
    "    emb_model,\n",
    "    wc,\n",
    "    chunk_batch_size=4\n",
    "):\n",
    "    \"\"\"\n",
    "    Batch text pipeline with minimal logic changes:\n",
    "      - same chunking + BERT filtering\n",
    "      - same generate_output settings + retry settings\n",
    "      - same valid_json checks\n",
    "      - same negation behavior (but batched)\n",
    "      - same merge_outputs per patient\n",
    "    Returns dict: {index: {\"text\": ..., \"image\": {}}}\n",
    "    \"\"\"\n",
    "\n",
    "    # temp per patient: para_id -> final_response\n",
    "    temp_per_patient = {index: {} for index, _ in batch}\n",
    "\n",
    "    # Collect jobs: each is (index, para_id, chunk)\n",
    "    jobs = []\n",
    "    for index, dt in batch:\n",
    "        text = data_input[index][\"clinical_note\"].lower()\n",
    "        if wc != 0:\n",
    "            all_chunks = chunking_documents(text, bert_tokenizer, bert_model, word_count=wc)\n",
    "        else:\n",
    "            chunk_batch_size = len(batch) ## each note is considered a chunk itself so use batch size instead\n",
    "            all_chunks = [text]\n",
    "\n",
    "        for para_id, chunk in enumerate(all_chunks):\n",
    "            chunk = chunk.replace(\"'\", \"\").replace('\"', \"\")\n",
    "\n",
    "            if len(all_chunks) > 1:\n",
    "                pred_label = predict_label(bert_tokenizer, bert_model, {\"text\": chunk})\n",
    "            else:\n",
    "                pred_label = \"INFORMATIVE\"\n",
    "\n",
    "            if pred_label == \"INFORMATIVE\":\n",
    "                jobs.append((index, para_id, chunk))\n",
    "\n",
    "    # --- First batched generation ---\n",
    "    prompts1 = [chunk for (_, _, chunk) in jobs]\n",
    "    outs1 = []\n",
    "    for i in range(0, len(jobs), chunk_batch_size):\n",
    "        sub_jobs = jobs[i:i+chunk_batch_size]\n",
    "        prompts = [chunk for (_,_,chunk) in sub_jobs]\n",
    "        outs = generate_output_batch(\n",
    "            model, tokenizer, prompts,\n",
    "            temperature=0.3,\n",
    "            max_new_tokens=3000,\n",
    "            device=device\n",
    "        )\n",
    "        outs1.extend(outs)\n",
    "    retry_jobs = []\n",
    "    ok_records = []  # (index, para_id, chunk, final_response, complete_check)\n",
    "\n",
    "    for (index, para_id, chunk), response in zip(jobs, outs1):\n",
    "        try:\n",
    "            final_response, complete_check = valid_json(response)\n",
    "            phenos = final_response.get(\"phenotypes\", {})\n",
    "            if not isinstance(phenos, dict) or len(phenos) == 0:\n",
    "                raise ValueError(\"Empty or invalid phenotype dict. Retry!\")\n",
    "            ok_records.append((index, para_id, chunk, final_response, complete_check))\n",
    "        except Exception:\n",
    "            retry_jobs.append((index, para_id, chunk))\n",
    "\n",
    "    # --- Retry batched generation (only failures) ---\n",
    "    if len(retry_jobs) > 0:\n",
    "        prompts2 = [chunk for (_, _, chunk) in retry_jobs]\n",
    "        outs2 = []\n",
    "        for i in range(0, len(retry_jobs), chunk_batch_size):\n",
    "            sub_jobs = retry_jobs[i:i+chunk_batch_size]\n",
    "            prompts = [chunk for (_,_,chunk) in sub_jobs]\n",
    "            outs = generate_output_batch(\n",
    "                model, tokenizer, prompts,\n",
    "                temperature=0.4,\n",
    "                max_new_tokens=4000,\n",
    "                device=device\n",
    "            )\n",
    "            outs2.extend(outs)\n",
    "        for (index, para_id, chunk), response in zip(retry_jobs, outs2):\n",
    "            try:\n",
    "                final_response, complete_check = valid_json(response)\n",
    "                phenos = final_response.get(\"phenotypes\", {})\n",
    "                if not isinstance(phenos, dict) or len(phenos) == 0:\n",
    "                    raise ValueError(\"Empty or invalid phenotype dict after retry. No retry!\")\n",
    "                ok_records.append((index, para_id, chunk, final_response, complete_check))\n",
    "            except Exception as e:\n",
    "                final_response = {\"error_response\": response}\n",
    "                final_response[\"pid\"] = data_input[index].get(\"pid\", data_input[index].get(\"pmid\", \"unknown\"))\n",
    "                temp_per_patient[index][para_id] = final_response\n",
    "\n",
    "    # Put successful generations into temp_per_patient (but don’t negation yet)\n",
    "    # Also collect for batched negation\n",
    "    neg_chunks = []\n",
    "    neg_points = []\n",
    "    neg_keys = []  # (index, para_id, complete_check)\n",
    "\n",
    "    for index, para_id, chunk, final_response, complete_check in ok_records:\n",
    "        if negation:\n",
    "            neg_chunks.append(chunk)\n",
    "            neg_points.append(final_response)\n",
    "            neg_keys.append((index, para_id, complete_check))\n",
    "        else:\n",
    "            final_response[\"filtered_phenotypes\"] = {}\n",
    "            temp_per_patient[index][para_id] = final_response\n",
    "\n",
    "    # --- Batched negation ---\n",
    "    if negation and len(neg_points) > 0:\n",
    "        neg_texts = []\n",
    "        for i in range(0, len(neg_points), chunk_batch_size):\n",
    "            sub_chunks = neg_chunks[i:i+chunk_batch_size]\n",
    "            sub_points = neg_points[i:i+chunk_batch_size]\n",
    "            outs = negation_detection_batch(\n",
    "                negation_model,\n",
    "                negation_tokenizer,\n",
    "                sub_chunks,\n",
    "                sub_points,\n",
    "                device=device,\n",
    "                max_new_tokens=6000\n",
    "            )\n",
    "            neg_texts.extend(outs)\n",
    "\n",
    "        for (index, para_id, complete_check), final_response, neg_text in zip(neg_keys, neg_points, neg_texts):\n",
    "            try:\n",
    "                negation_response = neg_text\n",
    "                final_response = process_negation(final_response, negation_response, complete_check, emb_model)\n",
    "            except:\n",
    "                final_response[\"filtered_phenotypes\"] = {}\n",
    "            temp_per_patient[index][para_id] = final_response\n",
    "\n",
    "    # --- Merge per patient exactly as before ---\n",
    "    batch_results = {}\n",
    "    for index, _ in batch:\n",
    "        if len(temp_per_patient[index]) > 1:\n",
    "            text_out = merge_outputs(temp_per_patient[index])\n",
    "        else:\n",
    "            temp_value = list(temp_per_patient[index].values())\n",
    "            if len(temp_value) > 0:\n",
    "                text_out = temp_value[0]\n",
    "            else:\n",
    "                text_out = {}\n",
    "        batch_results[index] = {\"text\": text_out, \"image\": {}}\n",
    "\n",
    "    return batch_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae51dd",
   "metadata": {},
   "source": [
    "## Set Up Your Input here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df38bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input & Output Directory\n",
    "input_dir = \"/home/nguyenqm/projects/PhenoGPT2/testing/phenotagger/input/GSC+/\"\n",
    "output_dir = \"/home/nguyenqm/projects/github/PhenoGPT2/phenogpt2_qwen3_ehr_8b_ft_nofilter/model/evaluations/GSC+_sample\"\n",
    "\n",
    "## Directory to the PhenoGPT2 fine-tuned weights\n",
    "model_dir = \"/home/nguyenqm/projects/github/PhenoGPT2/phenogpt2_qwen3_ehr_8b_ft_nofilter/model\"\n",
    "\n",
    "## Flash Attention helps faster inference and lower GPU memory but it may not work for ARM-based system. Use \"spda\" or \"eager\" instead\n",
    "attn_implementation='flash_attention_2'\n",
    "\n",
    "## Provide the directory to LoRA weights if available; otherwise set False\n",
    "lora=False\n",
    "\n",
    "## Specify if you want to remove false positives (highly recommended)\n",
    "negation=True\n",
    "\n",
    "## Specify the NEGATION model name\n",
    "negation_model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "## This is always on! (unless you want to run vision analysis only)\n",
    "text_only=True\n",
    "\n",
    "## This is always off! (unless you want to run vision analysis)\n",
    "vision_only=False\n",
    "\n",
    "## Specify vision model: llama-vision, qwen-vision, and llava-med\n",
    "vision='llama-vision'\n",
    "\n",
    "## batch size (how many sample notes are processed at once)\n",
    "batch_size = 7\n",
    "\n",
    "## chunk batch size (how many chunks per batch are processed at once; this will be ignored when no chunking)\n",
    "chunk_batch_size = 7\n",
    "\n",
    "## Chunking word per paragraph (recommend to use 300 for long clinical notes); otherwise keep it at 0!\n",
    "wc = 0\n",
    "\n",
    "## If you want to run multiple instances of results (mostly use for SLURM JOB ARRAY)\n",
    "index = 0\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    input=input_dir,\n",
    "    output=output_dir,\n",
    "    model_dir=model_dir,\n",
    "    lora=lora,\n",
    "    index=index,\n",
    "    batch_size=batch_size,\n",
    "    chunk_batch_size=chunk_batch_size,\n",
    "    negation=negation,\n",
    "    negation_model=negation_model_name,\n",
    "    attn_implementation=attn_implementation,\n",
    "    text_only=text_only,\n",
    "    vision_only=vision_only,\n",
    "    vision=vision,\n",
    "    wc=wc\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f43b62",
   "metadata": {},
   "source": [
    "## Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e00d8e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/399 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Load PhenoGPT2\n",
    "model, tokenizer = build_llm(args)\n",
    "## Load Negation pipeline\n",
    "negation_model, negation_tokenizer, emb_model = build_negation(args)\n",
    "\n",
    "## Run BERT model for filtering non-phenotypic chunks\n",
    "wc = args.wc\n",
    "if wc != 0:\n",
    "    bert_tokenizer, bert_model = bert_init(local_dir = \"./models/bert_filtering/\")\n",
    "else:\n",
    "    bert_tokenizer, bert_model = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad5d585",
   "metadata": {},
   "source": [
    "## Start PhenoGPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7f966ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start phenogpt2\n",
      "/home/nguyenqm/projects/github/PhenoGPT2/phenogpt2_qwen3_ehr_8b_ft_nofilter/model/evaluations/GSC+_sample\n",
      "/home/nguyenqm/projects/github/PhenoGPT2/phenogpt2_qwen3_ehr_8b_ft_nofilter/model/evaluations/GSC+_sample/phenogpt2_rep0.pkl\n",
      "use_vision: False\n"
     ]
    }
   ],
   "source": [
    "print('start phenogpt2')\n",
    "output_dir = args.output\n",
    "\n",
    "print(output_dir)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "## Process Input:\n",
    "data_input = read_input(args.input)\n",
    "\n",
    "# Load extracted results\n",
    "out_path = f\"{args.output}/phenogpt2_rep{args.index}.pkl\"\n",
    "print(out_path, flush=True)\n",
    "\n",
    "use_text, use_vision = infer_modes(args, data_input)\n",
    "\n",
    "phenogpt2_vision = build_vision(args, use_vision)\n",
    "\n",
    "i = args.index\n",
    "negation = args.negation\n",
    "all_responses = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4877060a",
   "metadata": {},
   "source": [
    "## Running PhenoGPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d372197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# DataLoader wrapper (prefetch)\n",
    "# ----------------------------\n",
    "allocated_cpus = os.cpu_count() or 1\n",
    "num_workers = max(1, min(allocated_cpus - 1, 4))\n",
    "\n",
    "dataset = PhenoGPT2Dataset(data_input)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True if (os.cpu_count() or 0) > 1 else False,\n",
    "    collate_fn=_collate,\n",
    "    prefetch_factor=4 if (os.cpu_count() or 0) > 1 else None,\n",
    ")\n",
    "\n",
    "seen=0\n",
    "for batch in tqdm(loader, desc='Running Batch'):\n",
    "\n",
    "    # --- TEXT (batched GPU) ---\n",
    "    if use_text:\n",
    "        batch_text_results = process_one_batch_text(\n",
    "            batch=batch,\n",
    "            data_input=data_input,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            bert_tokenizer=bert_tokenizer,\n",
    "            bert_model=bert_model,\n",
    "            negation=negation,\n",
    "            negation_model=negation_model,\n",
    "            negation_tokenizer=negation_tokenizer,\n",
    "            emb_model=emb_model,\n",
    "            wc=wc,\n",
    "            chunk_batch_size=args.chunk_batch_size\n",
    "        )\n",
    "    else:\n",
    "        batch_text_results = {index: {\"text\": {}, \"image\": {}} for index, _ in batch}\n",
    "\n",
    "    # --- VISION ---\n",
    "    if use_vision:\n",
    "        for index, dt in batch:\n",
    "            vision_phenotypes = phenogpt2_vision.generate_descriptions(dt['image'])\n",
    "            phen2hpo = generate_output(model, tokenizer, vision_phenotypes, temperature=0.4, max_new_tokens=1024, device=device)\n",
    "            phen2hpo = \"{'demographics': {'age': '\" + phen2hpo\n",
    "            phen2hpo = valid_json(phen2hpo)\n",
    "            phen2hpo = phen2hpo.get(\"phenotypes\", {})\n",
    "            try:\n",
    "                phen2hpo = {phen: hpo_dict['HPO_ID'] for phen, hpo_dict in phen2hpo.items()}\n",
    "            except:\n",
    "                phen2hpo = {}\n",
    "            batch_text_results[index][\"image\"] = phen2hpo\n",
    "    else:\n",
    "        for index, dt in batch:\n",
    "            batch_text_results[index][\"image\"] = {}\n",
    "\n",
    "    # --- commit results per patient index ---\n",
    "    for index, _ in batch:\n",
    "        all_responses[index] = batch_text_results[index]\n",
    "\n",
    "    #     if seen <= 10:\n",
    "    #         print(all_responses[index], flush=True)\n",
    "        seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45386b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{out_path}', 'wb') as f:\n",
    "    pickle.dump(all_responses, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phenogpt2_user2",
   "language": "python",
   "name": "phenogpt2_user2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
