import os
from datasets import load_dataset, Dataset#, load_from_disk
import datasets
import torch
from tokenizers import AddedToken, pre_tokenizers
from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM
import numpy as np
from transformers import DataCollatorForSeq2Seq
torch.backends.cuda.matmul.allow_tf32 = True
import pandas as pd
from sklearn.model_selection import train_test_split
from datetime import datetime
from tqdm.auto import tqdm
import gc, json
gc.collect()
torch.cuda.empty_cache()
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer_name = "Llama3_1/Meta-Llama-3.1-8B-Instruct" # Replace your tokenizer llama directory here
tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=False)
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
tokenizer.padding_side = "left"
def form_json_output(data_point):
    phenotype_dict = {v:k for k,v in data_point['output'].items()}
    return {"demographics":{'age':data_point['age'], 'sex': data_point['sex'], 'ethnicity': data_point['ethnicity'], 'race':data_point['race']}, "phenotypes":phenotype_dict}
def tokenize(prompt, add_eos_token=True, tokenizer = tokenizer):
    CUTOFF_LEN = 11000 ## Maximum token length for a single input text (roughly 9000 words)
    result = tokenizer(
        prompt,
        truncation=True,
        max_length=CUTOFF_LEN,
        padding=False,
        return_tensors=None,
    )
    if (
        result["input_ids"][-1] != tokenizer.eos_token_id
        and len(result["input_ids"]) < CUTOFF_LEN
        and add_eos_token
    ):
        result["input_ids"].append(tokenizer.eos_token_id)
        result["attention_mask"].append(1)

    result["labels"] = result["input_ids"].copy()

    return result
def generate_prompt(data_point):
    json_output = form_json_output(data_point)
    instruction = "You are a genetic counselor specializing in extracting demographic details and Human Phenotype Ontology (HPO) terms from text and generating a JSON object. Your task is to provide accurate and concise information without generating random answers. When demographic details or phenotype information is not explicitly mentioned in the input, use 'unknown' as the value."
    question = "Read the following input text and generate a JSON-formatted output with the following keys: demographics and phenotypes.For the demographics key, create a sub-dictionary with age, sex, ethnicity, and race as keys, and where applicable, imply the race from ethnicity or ethnicity from race. For the phenotype key, create a sub-dictionary where each HPO term is a key, and its corresponding HPO identifier is the value. If any information is unavailable, return 'unknown' for that field.\nInput: "
    base_prompt = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>

    {system_prompt}<|eot_id|>
    
    <|start_header_id|>user<|end_header_id|>

    {user_prompt}<|eot_id|>
    
    <|start_header_id|>assistant<|end_header_id|>
    {model_answer}<|eot_id|><|end_of_text|>"""
    
    prompt = base_prompt.format(system_prompt = instruction,
                                user_prompt = question + data_point['input'],
                                model_answer = "\n|==|Response|==|\n" + str(json_output)
                                )
    return prompt
def generate_and_tokenize_prompt(data_point): ## formulate the input text template and tokenize to numbers
    full_prompt = generate_prompt(data_point) # if just use raw text as input => for pretraining
    tokenized_full_prompt = tokenize(full_prompt)
    return tokenized_full_prompt
def defining_args():
    return """
    llama 3.1 8B
    finetuning to extract demographics details and HPO information
    with instruction prompt
    instruction = "You are a genetic counselor specializing in extracting demographic details and Human Phenotype Ontology (HPO) terms from text and generating a JSON object. Your task is to provide accurate and concise information without generating random answers. When demographic details or phenotype information is not explicitly mentioned in the input, use 'unknown' as the value."
    question = "Read the following input text and generate a JSON-formatted output with the following keys: demographics and phenotypes.For the demographics key, create a sub-dictionary with age, sex, ethnicity, and race as keys, and where applicable, imply the race from ethnicity or ethnicity from race. For the phenotype key, create a sub-dictionary where each HPO term is a key, and its corresponding HPO identifier is the value. If any information is unavailable, return 'unknown' for that field.\nInput: "
    no RAG
    model_answer = "\n|==|Response|==|\n" + json_output
    {'demographics':{'age':,'sex':,'ethnicity':,'race':}, 'phenotypes':{}}
    synthetic data generated by Llama 3.1 70B + HPO DB (plus phenotagger labels)
    phenogpt2_training_data_mix_syntheticNhpodb.json
    phenogpt2_val_data_mix_syntheticNhpodb.json
    """
def main():
    """
    Set training parameters and train model
    """
    with open('phenogpt2_training_data_mix_syntheticNhpodb.json', 'r') as f: # REPLACE YOUR TRAINING DATA HERE
        train_data = json.load(f)
    with open('phenogpt2_val_data_mix_syntheticNhpodb.json', 'r') as f: # REPLACE VALIDATION DATA HERE
        val_data = json.load(f)
    print(generate_prompt(train_data[0]))
    train_data = list(map(generate_and_tokenize_prompt,  tqdm(train_data, desc="Processing")))
    val_data = list(map(generate_and_tokenize_prompt, tqdm(val_data, desc="Processing")))
    train_data = {key: [item[key] for item in train_data] for key in train_data[0]}
    train_data = Dataset.from_dict(train_data)
    val_data = {key: [item[key] for item in val_data] for key in val_data[0]}
    val_data = Dataset.from_dict(val_data)
    model_name = "Llama3_1/Meta-Llama-3.1-8B-Instruct" # Replace your tokenizer llama directory here
    model=AutoModelForCausalLM.from_pretrained(model_name,do_sample=True, #quantization_config=quantization_config,
                                            attn_implementation="flash_attention_2",
                                            torch_dtype=torch.bfloat16, device_map = 'auto')
    with open('hpo_added_tokens.json', 'r') as f:
        name2hpo = json.load(f)
    all_hpo_ids = list(np.unique(list(name2hpo.values())))
    for hpo_id in tqdm(all_hpo_ids, desc = 'Adding Tokens'):
        #tokenizer.add_tokens(AddedToken(hpo_id, normalized=False,special=False))
        tokenizer.add_tokens([AddedToken(hpo_id, single_word=True, normalized=False,special=False)], special_tokens=False)
    #average_embedding = torch.mean(model.get_input_embeddings().weight, axis=0)

    model.resize_token_embeddings(len(tokenizer)) ## go along with tokenizer.pad_token is None
    model.config.pad_token_id = tokenizer.pad_token_id
    c= datetime.now()
    out_dir = os.getcwd() + '/phenogpt2_mix_syntheticNhpodb_tokenadded_' + str(c)
    os.makedirs(out_dir, exist_ok=True)
    out_dir_model = out_dir + '/model'
    with open(out_dir + '/params.txt', 'w') as f:
        f.write(defining_args())
    os.makedirs(out_dir_model, exist_ok=True)
    print(out_dir)
    training_args = TrainingArguments(
        output_dir=out_dir,
        warmup_ratio=0.3,
        optim="adamw_torch_fused",# use fused adamw optimizer, default parameters
        per_device_train_batch_size=20, #1
        gradient_accumulation_steps=8, #4
        gradient_checkpointing=True,
        gradient_checkpointing_kwargs={'use_reentrant': False},
        logging_strategy="steps",
        logging_steps=100,
        save_strategy="steps",  # Save the model checkpoint every logging step
        save_steps=1000,
        bf16=True,    # mixed precision
        tf32=True,
        do_eval=True,
        eval_strategy = 'steps',
        eval_steps = 100,
        #per_device_eval_batch_size=10,
        weight_decay=0.01,
        save_total_limit=2,
        push_to_hub=False,
        num_train_epochs=10,
    )
    trainer=Trainer(
        model=model,
        args=training_args,
        train_dataset=train_data,
        eval_dataset=val_data,
        data_collator=DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, return_tensors="pt", padding=True)
    )
    trainer.train()
    trainer.save_model(out_dir_model)
    print(os.system("nvidia-smi"))
if __name__ == "__main__":
    main()
