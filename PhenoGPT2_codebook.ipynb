{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae5b2e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "import os, sys, re, torch, json, glob, argparse, gc, ast, pickle, requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tokenizers import AddedToken\n",
    "from peft import PeftModel, PeftConfig\n",
    "from scripts.formatting_results import *\n",
    "from scripts.bert_filtering import *\n",
    "from scripts.negation import *\n",
    "from scripts.prompting import *\n",
    "from scripts.utils import *\n",
    "from scripts.llama_vision_engine import *\n",
    "#from scripts.llava_med_engine import *\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93ccb554",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Provide the model directory here\n",
    "#model_id = './models/phenogpt2/'\n",
    "model_id = '/home/nguyenqm/projects/github/PhenoGPT2/phenogpt2_EHR_L318B_text_FPFF/model'\n",
    "## Set True if your model is ft with LoRA, otherwise False\n",
    "lora = False\n",
    "## Provide the input dictionary file\n",
    "input_dir = './data/example/text_examples.json'\n",
    "#input_dir = './data/example/vision_examples.json'\n",
    "data_input = read_input(input_dir)\n",
    "## Replication (you can try to run three times for a sample then combine them but default to be 0)\n",
    "i = 0\n",
    "## Whether you want to remove negated findings (note that higher recall when negation = False)\n",
    "negation = False\n",
    "## If you don't want to split then keep 'wc' as 0; otherwise provide word size you want for each chunk\n",
    "wc = 0\n",
    "if wc != 0: \n",
    "    bert_tokenizer, bert_model = bert_init(local_dir = \"./models/bert_filtering/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffd87869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_vision: False\n"
     ]
    }
   ],
   "source": [
    "# Determine processing mode\n",
    "use_text = True\n",
    "use_vision = False\n",
    "vision_model = 'llama-vision'\n",
    "###\n",
    "# Vision model setup (only if vision is enabled)\n",
    "print(f\"use_vision: {use_vision}\")\n",
    "if use_vision:\n",
    "    #base_ckpt = \"/mnt/isilon/wang_lab/shared/LlaMA3.2-vision-instruct\"\n",
    "    phenogpt2_vision = LLaMA_Generator(os.getcwd() + \"/models/llama-vision-phenogpt2/\")#, base_ckpt)\n",
    "#     vision_model = vision_model.lower() if vision_model else \"llama-vision\"\n",
    "#     if vision_model == \"llava-med\":\n",
    "#         phenogpt2_vision = LLaVA_Generator(os.path.join(os.getcwd(), \"llava-med-phenogpt2\"))\n",
    "#     elif vision_model == \"llama-vision\":\n",
    "#         phenogpt2_vision = LLaMA_Generator(os.path.join(os.getcwd(), \"llama-vision-phenogpt2\"))\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported vision model '{vision_model}'. Use 'llava-med' or 'llama-vision'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46bbdeb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768d63ea0e8f459aa2f8e2674aff526b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(146672, 4096, padding_idx=128256)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=146672, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if lora:\n",
    "    peft_config = PeftConfig.from_pretrained(model_id)\n",
    "    base_model_name = peft_config.base_model_name_or_path or \"./models/hpo_aware_pretrain/\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(model, model_id)\n",
    "else: # either full finetuning or merged LoRA\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "tokenizer_id = model_id\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id, use_fast = True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e5e380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/isilon/wang_lab/quan/projects/github_official/PhenoGPT2/data/results/example_testing/\n"
     ]
    }
   ],
   "source": [
    "output = 'example_testing'\n",
    "output_dir = os.getcwd() + f\"/data/results/{output}/\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f637d14c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_responses = {}\n",
    "for index, dt in tqdm(data_input.items()):\n",
    "    all_responses[index] = {}\n",
    "    if use_text:\n",
    "        text = data_input[index]['clinical_note'].lower()\n",
    "        if wc != 0:\n",
    "            all_chunks = chunking_documents(text, bert_tokenizer, bert_model, word_count = wc)\n",
    "        else:\n",
    "            all_chunks = [text]\n",
    "        temp_response = {}\n",
    "        for para_id, chunk in enumerate(all_chunks):\n",
    "            if len(all_chunks) > 1:\n",
    "                pred_label = predict_label(bert_tokenizer, bert_model, {\"text\":chunk})\n",
    "            else: # in case users only want to use the whole note for testing\n",
    "                pred_label = 'INFORMATIVE'\n",
    "            if pred_label == 'INFORMATIVE':\n",
    "                raw_response = generate_output(model, tokenizer, chunk, temperature = 0.4, max_new_tokens = 5000, device = device)\n",
    "                response = \"{'demographics': {'age': '\" + raw_response\n",
    "                # Try first attempt\n",
    "                try:\n",
    "                    final_response = fix_and_parse_json(response)\n",
    "                    phenos = final_response.get(\"phenotypes\", {})\n",
    "                    if not isinstance(phenos, dict) or len(phenos) == 0:\n",
    "                        raise ValueError(\"Empty or invalid phenotype dict\")\n",
    "                except Exception:\n",
    "                    # Retry with alternative prompt\n",
    "                    try:\n",
    "                        raw_response = generate_output(model, tokenizer, chunk, temperature=0.4, max_new_tokens=6000, alternative_prompt=True, device = device)\n",
    "                        response = \"{'demographics': {'age': '\" + raw_response\n",
    "                        final_response = fix_and_parse_json(response)\n",
    "                        phenos = final_response.get(\"phenotypes\", {})\n",
    "                        if not isinstance(phenos, dict) or len(phenos) == 0:\n",
    "                            raise ValueError(\"Empty or invalid phenotype dict after retry\")\n",
    "                    except Exception:\n",
    "                        final_response = {'error_response': response}\n",
    "                        final_response['pid'] = data_input[index].get('pid', data_input[index].get('pmid', 'unknown'))\n",
    "                        temp_response[para_id] = final_response\n",
    "                        continue  # move to the next item\n",
    "                example_removed = ['cleft palate', 'seizures', 'dev delay'] ## here are phenotypes in one-shot in alternative prompt\n",
    "                temp_phenotypes = {k:v for k,v in final_response['phenotypes'].items() if (k not in example_removed) or (k in chunk)}\n",
    "                final_response['phenotypes'] = temp_phenotypes\n",
    "                if negation:\n",
    "                    phenotypes = list(final_response['phenotypes'].keys())\n",
    "                    phenotypes = [p.lower() for p in phenotypes]\n",
    "                    positive_phenotypes = remove_negation(model, tokenizer, chunk, phenotypes, device = device)\n",
    "                    try:\n",
    "                        phen_dict = {x:y for x,y in final_response['phenotypes'].items() if x in positive_phenotypes and \"HP:\" in y['HPO_ID']}\n",
    "                    except:\n",
    "                        phen_dict = {x:y for x,y in final_response['phenotypes'].items() if x in positive_phenotypes}\n",
    "                else:\n",
    "                    phen_dict = {}\n",
    "                final_response['filtered_phenotypes'] = phen_dict\n",
    "                if 'pid' in data_input[index]:\n",
    "                    final_response['pid'] = data_input[index]['pid']\n",
    "                else:\n",
    "                    final_response['pid'] = data_input[index]['pmid']\n",
    "                if 'demographics' in final_response.keys():\n",
    "                    if ('age' not in final_response['demographics'].keys()) or (final_response['demographics']['age'] == '10-year-old' and '10-year-old' not in chunk):\n",
    "                        final_response['demographics']['age'] = 'unknown'\n",
    "                    if 'sex' not in final_response['demographics'].keys():\n",
    "                        final_response['demographics']['sex'] = 'unknown'\n",
    "                    if 'ethnicity' not in final_response['demographics'].keys() or (final_response['demographics']['ethnicity'].lower() in ['vietnamese', 'vietnam'] and ('vietnamese' not in chunk or 'vietnam' not in chunk)):\n",
    "                        final_response['demographics']['ethnicity'] = 'unknown'\n",
    "                    if 'race' not in final_response['demographics'].keys():\n",
    "                        final_response['demographics']['race'] = 'unknown'\n",
    "                temp_response[para_id] = final_response\n",
    "        all_responses[index]['text'] = merge_outputs(temp_response)\n",
    "    else:\n",
    "        all_responses[index]['text'] = {}\n",
    "    if use_vision:\n",
    "        vision_phenotypes = phenogpt2_vision.generate_descriptions(dt['image'])\n",
    "        phen2hpo = generate_output(model, tokenizer, vision_phenotypes, temperature = 0.4, max_new_tokens = 1024, device = device)\n",
    "        phen2hpo = \"{'demographics': {'age': '\" + phen2hpo\n",
    "        phen2hpo = fix_and_parse_json(phen2hpo)\n",
    "        phen2hpo = phen2hpo.get(\"phenotypes\", {})\n",
    "        try:\n",
    "            phen2hpo = {phen:hpo_dict['HPO_ID'] for phen,hpo_dict in phen2hpo.items()}\n",
    "        except:\n",
    "            phen2hpo = {}\n",
    "        all_responses[index]['image'] = phen2hpo\n",
    "    else:\n",
    "        all_responses[index]['image'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f6c5fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{output_dir}phenogpt2_rep{i}.json', 'w') as f:\n",
    "    json.dump(all_responses, f, indent = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phenogpt2",
   "language": "python",
   "name": "phenogpt2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
